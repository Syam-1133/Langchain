{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0d489a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b89b7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Ingestion--From the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ba18344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x17eb03860>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://www.ibm.com/think/topics/langchain\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7af44c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat Is LangChain? | IBM\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                                   \\n\\n\\n\\n  \\n    What is LangChain?\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                               \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAI Agents\\n\\n\\n\\nWelcome\\n\\n\\n\\n\\n\\nCaret right\\n\\nIntroduction\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nAI agents vs AI assistants\\n\\n\\n\\n\\nAgentic AI\\n\\n\\n\\n\\nAgentic AI vs generative AI\\n\\n\\n\\n\\n\\nCaret right\\n\\nAI agent development\\n\\n\\n\\n\\nWhat is AI agent development?\\n\\n\\n\\n\\nAgentOps\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nTypes of AI agents\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nSimple reflex agent\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nComponents\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\nCaret right\\n\\nAgentic workflows\\n\\n\\n\\n\\nWhat are agentic workflows?\\n\\n\\n\\n\\nTutorial: Agentic workflows\\n\\n\\n\\n\\n\\n\\nCommunication\\n\\n\\n\\n\\nLearning\\n\\n\\n\\n\\nMemory\\n\\n\\n\\n\\nPerception\\n\\n\\n\\n\\nPlanning\\n\\n\\n\\n\\nReasoning\\n\\n\\n\\n\\n\\nCaret right\\n\\nTool calling\\n\\n\\n\\n\\nWhat is tool calling?\\n\\n\\n\\n\\nTutorial: Ollama tool calling\\n\\n\\n\\n\\nTutorial: LM Studio tool calling\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nArchitecture\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\nCaret right\\n\\nAI agent orchestration\\n\\n\\n\\n\\nWhat is agent orchestration?\\n\\n\\n\\n\\nTutorial: Agent orchestration\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nMulti-agent systems\\n\\n\\n\\n\\nWhat are multi-agent systems?\\n\\n\\n\\n\\nTutorial: crewAI multi-agent call analysis\\n\\n\\n\\n\\n\\n\\nMulti-agent collaboration\\n\\n\\n\\n\\n\\nCaret right\\n\\nReAct\\n\\n\\n\\n\\nWhat is ReAct?\\n\\n\\n\\n\\nTutorial: LangGraph ReAct agent\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nReWOO\\n\\n\\n\\n\\nWhat is ReWOO?\\n\\n\\n\\n\\nTutorial: ReWOO reasoning agent\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nProtocols\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\nCaret right\\n\\nAgent Communication Protocol (ACP)\\n\\n\\n\\n\\nWhat is ACP?\\n\\n\\n\\n\\nTutorial: ACP agent interoperability\\n\\n\\n\\n\\n\\n\\nAgent2Agent (A2A)\\n\\n\\n\\n\\n\\nCaret right\\n\\nModel Context Protocol (MCP)\\n\\n\\n\\n\\nWhat is MCP?\\n\\n\\n\\n\\nTutorial: MCP server\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nMulti-agent systems\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\nCaret right\\n\\nAgentic orchestration\\n\\n\\n\\n\\nWhat is agentic orchestration?\\n\\n\\n\\n\\nTutorial: Agent orchestration\\n\\n\\n\\n\\n\\n\\nMulti-agent collaboration\\n\\n\\n\\n\\nTutorial: crewAI multi-agent call analysis\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nFrameworks\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\nCaret right\\n\\nAutoGen\\n\\n\\n\\n\\nTutorial: AutoGen multi-agent RAG\\n\\n\\n\\n\\n\\n\\nAutoGPT\\n\\n\\n\\n\\n\\nCaret right\\n\\nBeeAI\\n\\n\\n\\n\\nWhat is BeeAI?\\n\\n\\n\\n\\nTutorial: BeeAI agentic contract management\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nChatDev\\n\\n\\n\\n\\nWhat is ChatDev?\\n\\n\\n\\n\\nTutorial: ChatDev ChatChain\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\ncrewAI\\n\\n\\n\\n\\nWhat is crewAI?\\n\\n\\n\\n\\nTutorial: crewAI retail shelf optimization\\n\\n\\n\\n\\n\\n\\nIBM watsonx agents\\n\\n\\n\\n\\n\\nCaret right\\n\\nLangGraph\\n\\n\\n\\n\\nWhat is LangGraph?\\n\\n\\n\\n\\nTutorial: LangGraph SQL agent\\n\\n\\n\\n\\nTutorial: LangGraph ReAct agent\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nLangChain\\n\\n\\n\\n\\nWhat is LangChain?\\n\\n\\n\\n\\nTutorial: LangChain agent\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nMetaGPT\\n\\n\\n\\n\\nWhat is MetaGPT?\\n\\n\\n\\n\\nTutorial: Multi-agent PRD automation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nGovernance\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nAI agent ethics\\n\\n\\n\\n\\n\\nCaret right\\n\\nAI agent evaluation\\n\\n\\n\\n\\nWhat is AI agent evaluation?\\n\\n\\n\\n\\nTutorial: AI agent evaluation\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nHuman-in-the-loop\\n\\n\\n\\n\\nTutorial: Human-in-the-loop\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nAgentic RAG\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nTutorial: LangChain agentic RAG \\n\\n\\n\\n\\n\\nCaret right\\n\\nAgentic chunking\\n\\n\\n\\n\\nWhat is agentic chunking?\\n\\n\\n\\n\\nTutorial: Agentic chunking for RAG\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nCorrective RAG\\n\\n\\n\\n\\nTutorial: Corrective RAG\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nUse cases / Applications\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nAutomation\\n\\n\\n\\n\\nCustomer service\\n\\n\\n\\n\\nFinance\\n\\n\\n\\n\\nHuman resources\\n\\n\\n\\n\\nMarketing\\n\\n\\n\\n\\nProcurement\\n\\n\\n\\n\\nSales\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Authors\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDave Bergmann\\n\\nStaff Writer, AI Models\\nIBM Think\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCole Stryker\\n\\nStaff Editor, AI Models\\nIBM Think\\n\\n\\n\\n\\n\\n\\n\\r\\n        What is LangChain?\\r\\n    \\n\\n\\n\\nLangChain is an open source orchestration framework for application development using large language models (LLMs). Available in both Python- and Javascript-based libraries, LangChain’s tools and APIs simplify the process of building LLM-driven applications like chatbots and AI agents.\\u202f\\n\\n\\nLangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows. LangChain’s module-based approach allows developers and data scientists to dynamically compare different prompts and even different foundation models with minimal need to rewrite code. This modular environment also allows for programs that use multiple LLMs: for example, an application that uses one LLM to interpret user queries and another LLM to author a response.\\nLaunched by Harrison Chase in October 2022, LangChain enjoyed a meteoric rise to prominence: as of June 2023, it was the single fastest-growing open source project on Github.1 Coinciding with the momentous launch of OpenAI’s ChatGPT the following month, LangChain has played a significant role in making generative AI\\xa0(genAI) more accessible to enthusiasts and startups in the wake of its widespread popularity. Advancements in accessibility for agentic AI are currently enabling a revolution in automation.\\nLangChain can facilitate most use cases for LLMs and natural language processing (NLP), like chatbots, intelligent search, question-answering, summarization services or even AI\\xa0agents capable of robotic process automation.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntegrations with LLMs\\n\\n\\nLLMs are not standalone applications: they are pre-trained statistical models that must be paired with an application (and, in some cases, specific data sources) in order to meet their purpose.\\nFor example, Chat-GPT is not an LLM: it is a chatbot application that, depending on the version you’ve chosen, uses the GPT-3.5 or GPT-4 language model. While it’s the GPT model that interprets the user’s input and composes a natural language response, it’s the application that (among other things) provides an interface for the user to type and read and a UX design that governs the chatbot experience. Even at the enterprise level, Chat-GPT is not the only application using the GPT model: Microsoft uses GPT-4 to power Bing Chat.\\nFurthermore, though foundation models (like those powering LLMs) are pre-trained on massive datasets, they are not omniscient. If a particular task requires access to specific contextual information, like internal documentation or domain expertise, LLMs must be connected to those external data sources. Even if you simply want your model to reflect real-time awareness of current events, it requires external information: a model’s internal data is only up-to-date through the time period during which it was pre-trained.\\nLikewise, if a given generative AI task requires access to external software workflows—for example, if you wanted your virtual agent to integrate with Slack—then you will need a way to integrate the LLM with the API for that software.\\nWhile these integrations can generally be achieved with fully manual code, orchestration frameworks such as LangChain and the IBM watsonx portfolio of artificial intelligence products greatly simplify the process. They also make it much easier to experiment with different LLMs to compare results, as different models can be swapped in and out with minimal changes to code.\\n\\n\\n\\r\\n        How does LangChain work?\\r\\n    \\n\\n\\n\\nAt LangChain’s core is a development environment that streamlines the programming of LLM applications through the use of\\xa0abstraction: the simplification of code by representing one or more complex processes as a named component that encapsulates all of its constituent steps.\\n\\n\\nAbstractions are a common element of everyday life and language. For example, “π” allows us to represent the ratio of the length of a circle’s circumference to that of its diameter without having to write out its infinite digits. Similarly, a thermostat allows us to control the temperature in our home without needing to understand the complex circuitry this entails—we only need to know how different thermostat settings translate to different temperatures.\\nLangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models. These modular components—like functions and object classes—serve as the building blocks of generative AI programs. They can be “chained” together to create applications, minimizing the amount of code and fine understanding required to execute complex NLP tasks. Though LangChain’s abstracted approach may limit the extent to which an expert programmer can finely customize an application, it empowers specialists and newcomers alike to quickly experiment and prototype.\\n\\n\\nImporting language models\\n\\n\\nNearly any LLM can be used in LangChain. Importing language models into LangChain is easy, provided you have an API key. The LLM class is designed to provide a standard interface for all models.\\nMost LLM providers will require you to create an account in order to receive an API key. Some of these APIs—particularly those for proprietary closed-source models, like those offered by OpenAI or Anthropic—may have associated costs.\\nMany open source models, like Meta AI’s LLaMa, Deepseek's Deepseek-LLM, IBM's Granite and Google’s Flan-T5, can be accessed through Hugging Face. IBM watsonx, through its partnership with Hugging Face, also offers a curated suite of open source models. Creating an account with either service will allow you to generate an API key for any of the models offered by that provider.\\nLangChain is not limited to out-of-the-box foundation models: the CustomLLM class\\xa0allows for custom LLM wrappers. Likewise, you can use the IBM watsonx APIs and Python SDK, which includes a LangChain integration, to build applications in LangChain with models that you’ve already trained or fine-tuned for your specific needs using the WatsonxLLM class (and that model’s specific project ID).\\n\\n\\nPrompt templates\\n\\n\\nPrompts are the instructions given to an LLM. The “art” of composing prompts that effectively provide the context necessary for the LLM to interpret input and structure output in the way most useful to you is often called prompt engineering.\\nThe PromptTemplate class in LangChain formalizes the composition of prompts without the need to manually hard code context and queries. Important elements of a prompt are likewise entered as formal classes, like input_variables. A prompt template can thus contain and reproduce context, instructions (like “do not use technical terms”), a set of examples to guide its responses (in what is called “few-shot prompting”), a specified output format or a standardized question to be answered.\\u202fYou can save and name an effectively structured prompt template and easily reuse it as needed.\\nThough these elements can all be manually coded, PromptTemplate modules empower smooth integration with other LangChain features, like the eponymous chains.\\n\\n\\nChains\\n\\n\\nAs its name implies, chains are the core of LangChain’s workflows. They combine LLMs with other components, creating applications by executing a sequence of functions.\\nThe most basic chain is\\xa0LLMChain. It simply calls a model and prompt template for that model. For example, imagine you saved a prompt as “ExamplePrompt” and wanted to run it against Flan-T5. You can import LLMChain from langchain.chains, then define\\xa0chain_example = LLMChain(llm = flan-t5, prompt = ExamplePrompt). To run the chain for a given input, you simply call\\xa0chain_example.run(“input”).\\nTo use the output of one function as the input for the next function, you can use SimpleSequentialChain. Each function could utilize different prompts, different tools, different parameters or even different models, depending on your specific needs.\\n\\n\\nIndexes\\n\\n\\nTo achieve certain tasks, LLMs will need access to specific external data sources not included in its training dataset, such as internal documents, emails or datasets. LangChain collectively refers to such external documentation as “indexes”.\\n\\n\\nDocument loaders\\n\\n\\nLangChain offers\\xa0a wide variety of document loaders for third party applications. This allows for easy importation of data from sources like file storage services (like Dropbox, Google Drive and Microsoft OneDrive), web content (like YouTube, PubMed or specific URLs), collaboration tools (like Airtable, Trello, Figma and Notion), databases (like Pandas, MongoDB and Microsoft), among many others.\\n\\n\\nVector databases\\n\\n\\nUnlike “traditional” structured databases,\\xa0vector databases\\xa0represent data points by converting them into\\xa0vector embeddings: numerical representations in the form of vectors with a fixed number of dimensions, often clustering related data points using\\xa0unsupervised learning methods. This enables low latency queries, even for massive datasets, which greatly increases efficiency. Vector embeddings also store each vector’s metadata, further enhancing search possibilities.\\nLangChain provides integrations for over 25 different embedding methods, as well as for over 50 different vector stores (both cloud-hosted and local).\\n\\n\\nText splitters\\xa0\\n\\n\\nTo increase speed and reduce computational demands, it’s often wise to split large text documents into smaller pieces. LangChain’s\\xa0TextSplitters\\xa0split text up into small, semantically meaningful chunks that can then be combined using methods and parameters of your choosing.\\n\\n\\nRetrieval\\n\\n\\nOnce external sources of knowledge have been connected, the model must be able to quickly retrieve and integrate relevant information as needed. Like watsonx, LangChain offers\\xa0retrieval augmented generation (RAG):\\xa0its\\xa0retriever\\xa0modules accept a string query as an input and return a list of\\xa0Document’s as output.\\nWith LangChain, we can also build agentic RAG systems.\\xa0In traditional RAG applications, the LLM is provided with a vector database to reference when forming its responses. In contrast, agentic AI applications are not restricted to only data retrieval. Agenic RAG can also encompass tools for tasks such as solving mathematical calculations, writing emails, performing data analysis and more.\\n\\n\\nMemory\\n\\n\\nLLMs, by default, do not have any long-term memory of previous interactions (unless that chat history is used as input for a query). LangChain solves this problem with simple utilities for adding memory to a system, with options ranging from retaining the entirety of all conversations to retaining a summarization of the conversation thus far to retaining the n\\xa0most recent exchanges.\\n\\n\\nTools\\n\\n\\nDespite their heralded power and versatility, LLMs have important limitations: namely, a lack of up-to-date information, a lack of domain-specific expertise and a general difficulty with math.\\nLangChain tools\\xa0are a set of functions that empower LangChain agents to interact with real-world information in order to expand or improve the services it can provide. Examples of prominent pre-built LangChain tools include:\\n\\nWolfram Alpha: provides access to powerful computational and data visualization functions, enabling sophisticated mathematical capabilities.\\nGoogle Search: provides access to Google Search, equipping applications and agents with real-time information.\\nOpenWeatherMap: fetches weather information.\\nWikipedia: provides efficient access to information from Wikipedia articles.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIndustry newsletter\\n\\n\\n\\nThe latest AI trends, brought to you by experts\\n\\n\\n\\nGet curated insights on the most important—and intriguing—AI news. Subscribe to our weekly Think newsletter. See the IBM Privacy Statement.\\n\\n\\n\\n\\nThank you! You are subscribed.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYour subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe here. Refer to our IBM Privacy Statement for more information.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        LangChain agents\\r\\n    \\n\\n\\n\\nWe can build an\\xa0agent\\xa0with the LangChain framework to give an LLM the ability to make decisions, use tools and complete complex tasks step-by-step, rather than just generating a single text response. Unlike a simple prompt-response interaction with just an LLM, an agent powered by LangChain can think, plan, execute a sequence of actions, learn and adapt.\\nLangChain provides a streamlined user experience with a ready-made, extensible framework for creating AI agents, so there’s no need to build new tool selection logic, reasoning loops (such as for ReAct agents), observation/action tracking or prompt orchestration and formatting.\\nThe specific LangChain packages, classes and methods vary depending on the AI platform you intend to use. Some key components of the WatsonxLLM class that allow for communication with watsonx.ai models using LangChain include:\\nlangchain_ibm: The package responsible for the LangChain IBM integration. It is necessary to install this package to use any of the following classes and methods.ibm_watsonx_ai: The library that allows connection to watsonx.ai services like IBM Cloud and IBM Cloud Pak for Data.APIClient: The main class of the ibm_watsonx_ai\\xa0library that manages the API service resources. The parameters include the API credentials and endpoint.WatsonxLLM: The wrapper for IBM watsonx.ai foundation models. This wrapper provides chain integration and is necessary to import. The parameters include the model ID, watsonx.ai API key, URL endpoint, project ID as well as any LLM parameters.ModelInference: The class that instantiates the model interface. The parameters include the model ID, watsonx.ai credentials, project ID, model parameters and more. Once instantiated, the model can then be passed into the class.invoke: The method that calls the model directly with a single prompt of string type.\\xa0generate:\\xa0The method that calls the model with multiple prompts of string type in a list.\\nAnother LangChain class for building AI agents with the integration of tool calling and chaining with\\xa0watsonx.ai models is ChatWatsonx. This class, which is leveraged in many of our tutorials, uses the bind_tools method to pass a list of tools to the LLM upon each iteration. These can include both custom and pre-built tools. To retrieve the AI agent response, the invoke method\\xa0can be used. Once the agent is invoked, the tool_calls attribute\\xa0of the response displays the name, arguments, id and type of each tool call made, if any.\\n\\n\\nLangGraph\\n\\n\\nLangGraph, created by LangChain,\\xa0is an open source AI agent framework that supports multi-agent orchestration and enables developers to build\\xa0agentic workflows\\xa0where different agents interact, specialize and collaborate.\\xa0\\nAt its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an\\xa0AI agent workflow.\\xa0Combined with the human-in-the-loop monitoring mechanism and a set of API and tool integrations, LangGraph provides users with a versatile platform for developing AI solutions and workflows including\\xa0chatbots, state graphs and\\xa0other agent-based systems.\\xa0\\nWith the langchain-mcp-adapters library, LangGraph agents can also use tools defined on model context protocol (MCP) servers. The mcp library allows users to build custom MCP servers as well. Essentially, MCP enables a secure connection between an AI system, such as an AI agent, and external tools. Thus, various LLMs can connect to the same tools and data sources given the standard MCP.\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n      AI Academy\\n  \\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n\\n\\n\\n  \\n    Why foundation models are a paradigm shift for AI\\n\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\nLearn about a new class of flexible, reusable AI models that can unlock new revenue, reduce costs and increase productivity, then use our guidebook to dive deeper.\\n\\n\\n\\n\\n\\n\\nGo to episode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        LangSmith\\r\\n    \\n\\n\\n\\nReleased in the fall of 2023, LangSmith aims to bridge the gap between the accessible prototyping capabilities that brought LangChain to prominence and building production-quality LLM applications.\\xa0\\nLangSmith provides tools to monitor, evaluate and debug applications, including the ability to automatically trace all model calls to spot errors and test performance under different model configurations. The use of LangSmith is not limited to applications built using the LangChain ecosystem. The evaluation of agent performance is done using LLM-as-a-judge evaluators. This observability\\xa0and these key metrics aim to optimize more robust, cost-efficient applications.\\xa0\\n\\n\\n\\n\\n\\n\\r\\n        Getting started with LangChain\\r\\n    \\n\\n\\n\\nLangChain is open source and free to use: source code is\\xa0available for download on Github.\\nLangChain can also be installed on Python with a simple pip command: pip install langchain. To install all LangChain dependencies (rather than only those you find necessary), you can run the command\\xa0pip install langchain[all].\\nMany step-by-step tutorials are provided by IBM including LangChain tool calling, agentic RAG, LLM agent orchestration,\\xa0agentic chunking and more.\\n\\n\\n\\r\\n        LangChain use cases\\r\\n    \\n\\n\\n\\nAI Applications made with LangChain provide great utility for a variety of use cases, from straightforward question-answering and text generation tasks to more complex solutions that use an LLM as a “reasoning engine.”\\n\\n\\nChatbots\\n\\n\\nChatbots are among the most intuitive uses of LLMs. LangChain can be used to provide proper context for the specific use of a chatbot, and to integrate chatbots into existing communication channels and workflows with their own APIs.\\n\\n\\nSummarization\\n\\n\\nLanguage models can be tasked with summarizing many types of text, from breaking down complex academic articles and transcripts to providing a digest of incoming emails.\\n\\n\\nQuestion answering\\n\\n\\nUsing specific documents or specialized knowledge bases (like Wolfram, arXiv or PubMed), LLMs can retrieve relevant information from storage and articulate helpful answers). If fine-tuned or properly prompted, some LLMs can answer many questions even without external information.\\n\\n\\nData augmentation\\n\\n\\nLLMs can be used to generate\\xa0synthetic data\\xa0for use in\\xa0machine learning. For example, an LLM can be trained to generate additional data samples that closely resemble the data points in a training dataset.\\n\\n\\nVirtual agents\\n\\n\\nIntegrated with the right workflows, LangChain’s Agent modules can use an LLM to autonomously determine next steps and take action using robotic process automation (RPA).\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Ebook\\n            \\n\\n                How to choose the right foundation model\\n            \\nLearn how to choose the right approach in preparing datasets and employing foundation models.\\n\\nRead the ebook\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n\\n\\n\\n  \\n    Resources\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                AI models\\n            \\n\\n                Explore IBM Granite\\n            \\nDiscover IBM® Granite™, our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.\\n\\nMeet Granite\\n\\n\\n\\n\\n\\n\\n\\n\\n                Ebook\\n            \\n\\n                How to choose the right foundation model\\n            \\nLearn how to select the most suitable AI foundation model for your use case.\\n\\nRead the ebook\\n\\n\\n\\n\\n\\n\\n\\n\\n                Article\\n            \\n\\n                Discover the power of LLMs\\n            \\nDive into IBM Developer articles, blogs and tutorials to deepen your knowledge of LLMs.\\n\\nExplore the articles\\n\\n\\n\\n\\n\\n\\n\\n\\n                Report\\n            \\n\\n                IBM is named a Leader in Data Science & Machine Learning\\n            \\nLearn why IBM has been recognized as a Leader in the 2025 Gartner® Magic Quadrant™ for Data Science and Machine Learning Platforms.\\n\\nRead the report\\n\\n\\n\\n\\n\\n\\n\\n\\n                Guide\\n            \\n\\n                The CEO’s guide to model optimization\\n            \\nLearn how to continually push teams to improve model performance and outpace the competition by using the latest AI techniques and infrastructure.\\n\\nRead the guide\\n\\n\\n\\n\\n\\n\\n\\n\\n                Report\\n            \\n\\n                A differentiated approach to AI foundation models\\n            \\nExplore the value of enterprise-grade foundation models that\\r\\nprovide trust, performance and cost-effective benefits to\\r\\nall industries.\\n\\nRead the report\\n\\n\\n\\n\\n\\n\\n\\n\\n                Ebook\\n            \\n\\n                Unlock the power of generative AI and ML\\n            \\nLearn how to incorporate generative AI, machine learning and foundation models into your business operations for improved performance.\\n\\nRead the ebook\\n\\n\\n\\n\\n\\n\\n\\n\\n                Report\\n            \\n\\n                AI in Action 2024\\n            \\nRead about 2,000 organizations we surveyed about their AI initiatives to discover what's working, what's not and how you can get ahead.\\n\\nRead the report\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n        \\n\\n     \\n    Related solutions\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Foundation models\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nExplore Granite 3.2 and the IBM library of foundation models in the watsonx portfolio to scale generative AI for your business with confidence.\\n\\n\\n\\n\\nExplore watsonx.ai\\n                \\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Artificial intelligence solutions\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nPut AI to work in your business with IBM’s industry-leading AI expertise and portfolio of solutions at your side.\\n\\n\\n\\n\\nExplore AI solutions\\n                \\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    AI consulting and services\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nReinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value.\\n\\n\\n\\n\\nExplore AI services\\n                \\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTake the next step\\n\\n\\n\\n\\nExplore the IBM library of foundation models in the IBM watsonx portfolio to scale generative AI for your business with confidence.\\n\\n\\n\\n\\n\\n\\nExplore watsonx.ai\\n\\n\\n\\n\\n\\nExplore AI solutions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de520f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data--> Docs-->Divide our Docuemnts into chunks dcouments-->text-->vectors-->Vector Embeddings--->Vector Store DB\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82c50b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='What Is LangChain? | IBM\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                                   \\n\\n\\n\\n  \\n    What is LangChain?\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                               \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAI Agents\\n\\n\\n\\nWelcome\\n\\n\\n\\n\\n\\nCaret right\\n\\nIntroduction\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nAI agents vs AI assistants\\n\\n\\n\\n\\nAgentic AI\\n\\n\\n\\n\\nAgentic AI vs generative AI\\n\\n\\n\\n\\n\\nCaret right\\n\\nAI agent development\\n\\n\\n\\n\\nWhat is AI agent development?\\n\\n\\n\\n\\nAgentOps\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nTypes of AI agents\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nSimple reflex agent\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nComponents\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\nCaret right\\n\\nAgentic workflows\\n\\n\\n\\n\\nWhat are agentic workflows?\\n\\n\\n\\n\\nTutorial: Agentic workflows\\n\\n\\n\\n\\n\\n\\nCommunication\\n\\n\\n\\n\\nLearning\\n\\n\\n\\n\\nMemory\\n\\n\\n\\n\\nPerception\\n\\n\\n\\n\\nPlanning\\n\\n\\n\\n\\nReasoning\\n\\n\\n\\n\\n\\nCaret right\\n\\nTool calling\\n\\n\\n\\n\\nWhat is tool calling?\\n\\n\\n\\n\\nTutorial: Ollama tool calling\\n\\n\\n\\n\\nTutorial: LM Studio tool calling\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nArchitecture'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Planning\\n\\n\\n\\n\\nReasoning\\n\\n\\n\\n\\n\\nCaret right\\n\\nTool calling\\n\\n\\n\\n\\nWhat is tool calling?\\n\\n\\n\\n\\nTutorial: Ollama tool calling\\n\\n\\n\\n\\nTutorial: LM Studio tool calling\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nArchitecture\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\nCaret right\\n\\nAI agent orchestration\\n\\n\\n\\n\\nWhat is agent orchestration?\\n\\n\\n\\n\\nTutorial: Agent orchestration\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nMulti-agent systems\\n\\n\\n\\n\\nWhat are multi-agent systems?\\n\\n\\n\\n\\nTutorial: crewAI multi-agent call analysis\\n\\n\\n\\n\\n\\n\\nMulti-agent collaboration\\n\\n\\n\\n\\n\\nCaret right\\n\\nReAct\\n\\n\\n\\n\\nWhat is ReAct?\\n\\n\\n\\n\\nTutorial: LangGraph ReAct agent\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nReWOO\\n\\n\\n\\n\\nWhat is ReWOO?\\n\\n\\n\\n\\nTutorial: ReWOO reasoning agent\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nProtocols\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\nCaret right\\n\\nAgent Communication Protocol (ACP)\\n\\n\\n\\n\\nWhat is ACP?\\n\\n\\n\\n\\nTutorial: ACP agent interoperability\\n\\n\\n\\n\\n\\n\\nAgent2Agent (A2A)\\n\\n\\n\\n\\n\\nCaret right\\n\\nModel Context Protocol (MCP)\\n\\n\\n\\n\\nWhat is MCP?\\n\\n\\n\\n\\nTutorial: MCP server\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nMulti-agent systems\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\nCaret right\\n\\nAgentic orchestration'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Caret right\\n\\nModel Context Protocol (MCP)\\n\\n\\n\\n\\nWhat is MCP?\\n\\n\\n\\n\\nTutorial: MCP server\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nMulti-agent systems\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\nCaret right\\n\\nAgentic orchestration\\n\\n\\n\\n\\nWhat is agentic orchestration?\\n\\n\\n\\n\\nTutorial: Agent orchestration\\n\\n\\n\\n\\n\\n\\nMulti-agent collaboration\\n\\n\\n\\n\\nTutorial: crewAI multi-agent call analysis\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nFrameworks\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\nCaret right\\n\\nAutoGen\\n\\n\\n\\n\\nTutorial: AutoGen multi-agent RAG\\n\\n\\n\\n\\n\\n\\nAutoGPT\\n\\n\\n\\n\\n\\nCaret right\\n\\nBeeAI\\n\\n\\n\\n\\nWhat is BeeAI?\\n\\n\\n\\n\\nTutorial: BeeAI agentic contract management\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nChatDev\\n\\n\\n\\n\\nWhat is ChatDev?\\n\\n\\n\\n\\nTutorial: ChatDev ChatChain\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\ncrewAI\\n\\n\\n\\n\\nWhat is crewAI?\\n\\n\\n\\n\\nTutorial: crewAI retail shelf optimization\\n\\n\\n\\n\\n\\n\\nIBM watsonx agents\\n\\n\\n\\n\\n\\nCaret right\\n\\nLangGraph\\n\\n\\n\\n\\nWhat is LangGraph?\\n\\n\\n\\n\\nTutorial: LangGraph SQL agent\\n\\n\\n\\n\\nTutorial: LangGraph ReAct agent\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nLangChain\\n\\n\\n\\n\\nWhat is LangChain?\\n\\n\\n\\n\\nTutorial: LangChain agent\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nMetaGPT\\n\\n\\n\\n\\nWhat is MetaGPT?'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Tutorial: LangGraph ReAct agent\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nLangChain\\n\\n\\n\\n\\nWhat is LangChain?\\n\\n\\n\\n\\nTutorial: LangChain agent\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nMetaGPT\\n\\n\\n\\n\\nWhat is MetaGPT?\\n\\n\\n\\n\\nTutorial: Multi-agent PRD automation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nGovernance\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nAI agent ethics\\n\\n\\n\\n\\n\\nCaret right\\n\\nAI agent evaluation\\n\\n\\n\\n\\nWhat is AI agent evaluation?\\n\\n\\n\\n\\nTutorial: AI agent evaluation\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nHuman-in-the-loop\\n\\n\\n\\n\\nTutorial: Human-in-the-loop\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nAgentic RAG\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nTutorial: LangChain agentic RAG \\n\\n\\n\\n\\n\\nCaret right\\n\\nAgentic chunking\\n\\n\\n\\n\\nWhat is agentic chunking?\\n\\n\\n\\n\\nTutorial: Agentic chunking for RAG\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nCorrective RAG\\n\\n\\n\\n\\nTutorial: Corrective RAG\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nUse cases / Applications\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nAutomation\\n\\n\\n\\n\\nCustomer service\\n\\n\\n\\n\\nFinance\\n\\n\\n\\n\\nHuman resources\\n\\n\\n\\n\\nMarketing\\n\\n\\n\\n\\nProcurement\\n\\n\\n\\n\\nSales\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Authors\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDave Bergmann\\n\\nStaff Writer, AI Models\\nIBM Think'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Finance\\n\\n\\n\\n\\nHuman resources\\n\\n\\n\\n\\nMarketing\\n\\n\\n\\n\\nProcurement\\n\\n\\n\\n\\nSales\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Authors\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDave Bergmann\\n\\nStaff Writer, AI Models\\nIBM Think\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCole Stryker\\n\\nStaff Editor, AI Models\\nIBM Think\\n\\n\\n\\n\\n\\n\\n\\r\\n        What is LangChain?\\r\\n    \\n\\n\\n\\nLangChain is an open source orchestration framework for application development using large language models (LLMs). Available in both Python- and Javascript-based libraries, LangChain’s tools and APIs simplify the process of building LLM-driven applications like chatbots and AI agents.'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='LangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows. LangChain’s module-based approach allows developers and data scientists to dynamically compare different prompts and even different foundation models with minimal need to rewrite code. This modular environment also allows for programs that use multiple LLMs: for example, an application that uses one LLM to interpret user queries and another LLM to author a response.'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Launched by Harrison Chase in October 2022, LangChain enjoyed a meteoric rise to prominence: as of June 2023, it was the single fastest-growing open source project on Github.1 Coinciding with the momentous launch of OpenAI’s ChatGPT the following month, LangChain has played a significant role in making generative AI\\xa0(genAI) more accessible to enthusiasts and startups in the wake of its widespread popularity. Advancements in accessibility for agentic AI are currently enabling a revolution in automation.\\nLangChain can facilitate most use cases for LLMs and natural language processing (NLP), like chatbots, intelligent search, question-answering, summarization services or even AI\\xa0agents capable of robotic process automation.'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Integrations with LLMs'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='LLMs are not standalone applications: they are pre-trained statistical models that must be paired with an application (and, in some cases, specific data sources) in order to meet their purpose.\\nFor example, Chat-GPT is not an LLM: it is a chatbot application that, depending on the version you’ve chosen, uses the GPT-3.5 or GPT-4 language model. While it’s the GPT model that interprets the user’s input and composes a natural language response, it’s the application that (among other things) provides an interface for the user to type and read and a UX design that governs the chatbot experience. Even at the enterprise level, Chat-GPT is not the only application using the GPT model: Microsoft uses GPT-4 to power Bing Chat.'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Furthermore, though foundation models (like those powering LLMs) are pre-trained on massive datasets, they are not omniscient. If a particular task requires access to specific contextual information, like internal documentation or domain expertise, LLMs must be connected to those external data sources. Even if you simply want your model to reflect real-time awareness of current events, it requires external information: a model’s internal data is only up-to-date through the time period during which it was pre-trained.\\nLikewise, if a given generative AI task requires access to external software workflows—for example, if you wanted your virtual agent to integrate with Slack—then you will need a way to integrate the LLM with the API for that software.'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='While these integrations can generally be achieved with fully manual code, orchestration frameworks such as LangChain and the IBM watsonx portfolio of artificial intelligence products greatly simplify the process. They also make it much easier to experiment with different LLMs to compare results, as different models can be swapped in and out with minimal changes to code.'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='How does LangChain work?\\r\\n    \\n\\n\\n\\nAt LangChain’s core is a development environment that streamlines the programming of LLM applications through the use of\\xa0abstraction: the simplification of code by representing one or more complex processes as a named component that encapsulates all of its constituent steps.'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Abstractions are a common element of everyday life and language. For example, “π” allows us to represent the ratio of the length of a circle’s circumference to that of its diameter without having to write out its infinite digits. Similarly, a thermostat allows us to control the temperature in our home without needing to understand the complex circuitry this entails—we only need to know how different thermostat settings translate to different temperatures.'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='LangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models. These modular components—like functions and object classes—serve as the building blocks of generative AI programs. They can be “chained” together to create applications, minimizing the amount of code and fine understanding required to execute complex NLP tasks. Though LangChain’s abstracted approach may limit the extent to which an expert programmer can finely customize an application, it empowers specialists and newcomers alike to quickly experiment and prototype.'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Importing language models'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content=\"Nearly any LLM can be used in LangChain. Importing language models into LangChain is easy, provided you have an API key. The LLM class is designed to provide a standard interface for all models.\\nMost LLM providers will require you to create an account in order to receive an API key. Some of these APIs—particularly those for proprietary closed-source models, like those offered by OpenAI or Anthropic—may have associated costs.\\nMany open source models, like Meta AI’s LLaMa, Deepseek's Deepseek-LLM, IBM's Granite and Google’s Flan-T5, can be accessed through Hugging Face. IBM watsonx, through its partnership with Hugging Face, also offers a curated suite of open source models. Creating an account with either service will allow you to generate an API key for any of the models offered by that provider.\"),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='LangChain is not limited to out-of-the-box foundation models: the CustomLLM class\\xa0allows for custom LLM wrappers. Likewise, you can use the IBM watsonx APIs and Python SDK, which includes a LangChain integration, to build applications in LangChain with models that you’ve already trained or fine-tuned for your specific needs using the WatsonxLLM class (and that model’s specific project ID).'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Prompt templates'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Prompts are the instructions given to an LLM. The “art” of composing prompts that effectively provide the context necessary for the LLM to interpret input and structure output in the way most useful to you is often called prompt engineering.\\nThe PromptTemplate class in LangChain formalizes the composition of prompts without the need to manually hard code context and queries. Important elements of a prompt are likewise entered as formal classes, like input_variables. A prompt template can thus contain and reproduce context, instructions (like “do not use technical terms”), a set of examples to guide its responses (in what is called “few-shot prompting”), a specified output format or a standardized question to be answered.\\u202fYou can save and name an effectively structured prompt template and easily reuse it as needed.\\nThough these elements can all be manually coded, PromptTemplate modules empower smooth integration with other LangChain features, like the eponymous chains.\\n\\n\\nChains'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Chains\\n\\n\\nAs its name implies, chains are the core of LangChain’s workflows. They combine LLMs with other components, creating applications by executing a sequence of functions.\\nThe most basic chain is\\xa0LLMChain. It simply calls a model and prompt template for that model. For example, imagine you saved a prompt as “ExamplePrompt” and wanted to run it against Flan-T5. You can import LLMChain from langchain.chains, then define\\xa0chain_example = LLMChain(llm = flan-t5, prompt = ExamplePrompt). To run the chain for a given input, you simply call\\xa0chain_example.run(“input”).\\nTo use the output of one function as the input for the next function, you can use SimpleSequentialChain. Each function could utilize different prompts, different tools, different parameters or even different models, depending on your specific needs.\\n\\n\\nIndexes'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Indexes\\n\\n\\nTo achieve certain tasks, LLMs will need access to specific external data sources not included in its training dataset, such as internal documents, emails or datasets. LangChain collectively refers to such external documentation as “indexes”.\\n\\n\\nDocument loaders\\n\\n\\nLangChain offers\\xa0a wide variety of document loaders for third party applications. This allows for easy importation of data from sources like file storage services (like Dropbox, Google Drive and Microsoft OneDrive), web content (like YouTube, PubMed or specific URLs), collaboration tools (like Airtable, Trello, Figma and Notion), databases (like Pandas, MongoDB and Microsoft), among many others.\\n\\n\\nVector databases'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Vector databases\\n\\n\\nUnlike “traditional” structured databases,\\xa0vector databases\\xa0represent data points by converting them into\\xa0vector embeddings: numerical representations in the form of vectors with a fixed number of dimensions, often clustering related data points using\\xa0unsupervised learning methods. This enables low latency queries, even for massive datasets, which greatly increases efficiency. Vector embeddings also store each vector’s metadata, further enhancing search possibilities.\\nLangChain provides integrations for over 25 different embedding methods, as well as for over 50 different vector stores (both cloud-hosted and local).\\n\\n\\nText splitters\\xa0\\n\\n\\nTo increase speed and reduce computational demands, it’s often wise to split large text documents into smaller pieces. LangChain’s\\xa0TextSplitters\\xa0split text up into small, semantically meaningful chunks that can then be combined using methods and parameters of your choosing.\\n\\n\\nRetrieval'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Retrieval\\n\\n\\nOnce external sources of knowledge have been connected, the model must be able to quickly retrieve and integrate relevant information as needed. Like watsonx, LangChain offers\\xa0retrieval augmented generation (RAG):\\xa0its\\xa0retriever\\xa0modules accept a string query as an input and return a list of\\xa0Document’s as output.\\nWith LangChain, we can also build agentic RAG systems.\\xa0In traditional RAG applications, the LLM is provided with a vector database to reference when forming its responses. In contrast, agentic AI applications are not restricted to only data retrieval. Agenic RAG can also encompass tools for tasks such as solving mathematical calculations, writing emails, performing data analysis and more.\\n\\n\\nMemory'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Memory\\n\\n\\nLLMs, by default, do not have any long-term memory of previous interactions (unless that chat history is used as input for a query). LangChain solves this problem with simple utilities for adding memory to a system, with options ranging from retaining the entirety of all conversations to retaining a summarization of the conversation thus far to retaining the n\\xa0most recent exchanges.\\n\\n\\nTools\\n\\n\\nDespite their heralded power and versatility, LLMs have important limitations: namely, a lack of up-to-date information, a lack of domain-specific expertise and a general difficulty with math.\\nLangChain tools\\xa0are a set of functions that empower LangChain agents to interact with real-world information in order to expand or improve the services it can provide. Examples of prominent pre-built LangChain tools include:'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Wolfram Alpha: provides access to powerful computational and data visualization functions, enabling sophisticated mathematical capabilities.\\nGoogle Search: provides access to Google Search, equipping applications and agents with real-time information.\\nOpenWeatherMap: fetches weather information.\\nWikipedia: provides efficient access to information from Wikipedia articles.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIndustry newsletter\\n\\n\\n\\nThe latest AI trends, brought to you by experts\\n\\n\\n\\nGet curated insights on the most important—and intriguing—AI news. Subscribe to our weekly Think newsletter. See the IBM Privacy Statement.\\n\\n\\n\\n\\nThank you! You are subscribed.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYour subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe here. Refer to our IBM Privacy Statement for more information.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        LangChain agents'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='We can build an\\xa0agent\\xa0with the LangChain framework to give an LLM the ability to make decisions, use tools and complete complex tasks step-by-step, rather than just generating a single text response. Unlike a simple prompt-response interaction with just an LLM, an agent powered by LangChain can think, plan, execute a sequence of actions, learn and adapt.\\nLangChain provides a streamlined user experience with a ready-made, extensible framework for creating AI agents, so there’s no need to build new tool selection logic, reasoning loops (such as for ReAct agents), observation/action tracking or prompt orchestration and formatting.\\nThe specific LangChain packages, classes and methods vary depending on the AI platform you intend to use. Some key components of the WatsonxLLM class that allow for communication with watsonx.ai models using LangChain include:'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='langchain_ibm: The package responsible for the LangChain IBM integration. It is necessary to install this package to use any of the following classes and methods.ibm_watsonx_ai: The library that allows connection to watsonx.ai services like IBM Cloud and IBM Cloud Pak for Data.APIClient: The main class of the ibm_watsonx_ai\\xa0library that manages the API service resources. The parameters include the API credentials and endpoint.WatsonxLLM: The wrapper for IBM watsonx.ai foundation models. This wrapper provides chain integration and is necessary to import. The parameters include the model ID, watsonx.ai API key, URL endpoint, project ID as well as any LLM parameters.ModelInference: The class that instantiates the model interface. The parameters include the model ID, watsonx.ai credentials, project ID, model parameters and more. Once instantiated, the model can then be passed into the class.invoke: The method that calls the model directly with a single prompt of string type.\\xa0generate:\\xa0The'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='ID, model parameters and more. Once instantiated, the model can then be passed into the class.invoke: The method that calls the model directly with a single prompt of string type.\\xa0generate:\\xa0The method that calls the model with multiple prompts of string type in a list.'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Another LangChain class for building AI agents with the integration of tool calling and chaining with\\xa0watsonx.ai models is ChatWatsonx. This class, which is leveraged in many of our tutorials, uses the bind_tools method to pass a list of tools to the LLM upon each iteration. These can include both custom and pre-built tools. To retrieve the AI agent response, the invoke method\\xa0can be used. Once the agent is invoked, the tool_calls attribute\\xa0of the response displays the name, arguments, id and type of each tool call made, if any.'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='LangGraph'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='LangGraph, created by LangChain,\\xa0is an open source AI agent framework that supports multi-agent orchestration and enables developers to build\\xa0agentic workflows\\xa0where different agents interact, specialize and collaborate.\\xa0\\nAt its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an\\xa0AI agent workflow.\\xa0Combined with the human-in-the-loop monitoring mechanism and a set of API and tool integrations, LangGraph provides users with a versatile platform for developing AI solutions and workflows including\\xa0chatbots, state graphs and\\xa0other agent-based systems.'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='With the langchain-mcp-adapters library, LangGraph agents can also use tools defined on model context protocol (MCP) servers. The mcp library allows users to build custom MCP servers as well. Essentially, MCP enables a secure connection between an AI system, such as an AI agent, and external tools. Thus, various LLMs can connect to the same tools and data sources given the standard MCP.'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='AI Academy\\n  \\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n\\n\\n\\n  \\n    Why foundation models are a paradigm shift for AI\\n\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\nLearn about a new class of flexible, reusable AI models that can unlock new revenue, reduce costs and increase productivity, then use our guidebook to dive deeper.\\n\\n\\n\\n\\n\\n\\nGo to episode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        LangSmith'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Go to episode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        LangSmith\\r\\n    \\n\\n\\n\\nReleased in the fall of 2023, LangSmith aims to bridge the gap between the accessible prototyping capabilities that brought LangChain to prominence and building production-quality LLM applications.\\xa0\\nLangSmith provides tools to monitor, evaluate and debug applications, including the ability to automatically trace all model calls to spot errors and test performance under different model configurations. The use of LangSmith is not limited to applications built using the LangChain ecosystem. The evaluation of agent performance is done using LLM-as-a-judge evaluators. This observability\\xa0and these key metrics aim to optimize more robust, cost-efficient applications.\\xa0\\n\\n\\n\\n\\n\\n\\r\\n        Getting started with LangChain'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Getting started with LangChain\\r\\n    \\n\\n\\n\\nLangChain is open source and free to use: source code is\\xa0available for download on Github.\\nLangChain can also be installed on Python with a simple pip command: pip install langchain. To install all LangChain dependencies (rather than only those you find necessary), you can run the command\\xa0pip install langchain[all].\\nMany step-by-step tutorials are provided by IBM including LangChain tool calling, agentic RAG, LLM agent orchestration,\\xa0agentic chunking and more.\\n\\n\\n\\r\\n        LangChain use cases\\r\\n    \\n\\n\\n\\nAI Applications made with LangChain provide great utility for a variety of use cases, from straightforward question-answering and text generation tasks to more complex solutions that use an LLM as a “reasoning engine.”\\n\\n\\nChatbots'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Chatbots\\n\\n\\nChatbots are among the most intuitive uses of LLMs. LangChain can be used to provide proper context for the specific use of a chatbot, and to integrate chatbots into existing communication channels and workflows with their own APIs.\\n\\n\\nSummarization\\n\\n\\nLanguage models can be tasked with summarizing many types of text, from breaking down complex academic articles and transcripts to providing a digest of incoming emails.\\n\\n\\nQuestion answering\\n\\n\\nUsing specific documents or specialized knowledge bases (like Wolfram, arXiv or PubMed), LLMs can retrieve relevant information from storage and articulate helpful answers). If fine-tuned or properly prompted, some LLMs can answer many questions even without external information.\\n\\n\\nData augmentation\\n\\n\\nLLMs can be used to generate\\xa0synthetic data\\xa0for use in\\xa0machine learning. For example, an LLM can be trained to generate additional data samples that closely resemble the data points in a training dataset.\\n\\n\\nVirtual agents'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Virtual agents\\n\\n\\nIntegrated with the right workflows, LangChain’s Agent modules can use an LLM to autonomously determine next steps and take action using robotic process automation (RPA).\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Ebook\\n            \\n\\n                How to choose the right foundation model\\n            \\nLearn how to choose the right approach in preparing datasets and employing foundation models.\\n\\nRead the ebook\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n\\n\\n\\n  \\n    Resources\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                AI models\\n            \\n\\n                Explore IBM Granite\\n            \\nDiscover IBM® Granite™, our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.\\n\\nMeet Granite\\n\\n\\n\\n\\n\\n\\n\\n\\n                Ebook'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Meet Granite\\n\\n\\n\\n\\n\\n\\n\\n\\n                Ebook\\n            \\n\\n                How to choose the right foundation model\\n            \\nLearn how to select the most suitable AI foundation model for your use case.\\n\\nRead the ebook\\n\\n\\n\\n\\n\\n\\n\\n\\n                Article\\n            \\n\\n                Discover the power of LLMs\\n            \\nDive into IBM Developer articles, blogs and tutorials to deepen your knowledge of LLMs.\\n\\nExplore the articles\\n\\n\\n\\n\\n\\n\\n\\n\\n                Report\\n            \\n\\n                IBM is named a Leader in Data Science & Machine Learning\\n            \\nLearn why IBM has been recognized as a Leader in the 2025 Gartner® Magic Quadrant™ for Data Science and Machine Learning Platforms.\\n\\nRead the report\\n\\n\\n\\n\\n\\n\\n\\n\\n                Guide\\n            \\n\\n                The CEO’s guide to model optimization\\n            \\nLearn how to continually push teams to improve model performance and outpace the competition by using the latest AI techniques and infrastructure.\\n\\nRead the guide'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content=\"Read the guide\\n\\n\\n\\n\\n\\n\\n\\n\\n                Report\\n            \\n\\n                A differentiated approach to AI foundation models\\n            \\nExplore the value of enterprise-grade foundation models that\\r\\nprovide trust, performance and cost-effective benefits to\\r\\nall industries.\\n\\nRead the report\\n\\n\\n\\n\\n\\n\\n\\n\\n                Ebook\\n            \\n\\n                Unlock the power of generative AI and ML\\n            \\nLearn how to incorporate generative AI, machine learning and foundation models into your business operations for improved performance.\\n\\nRead the ebook\\n\\n\\n\\n\\n\\n\\n\\n\\n                Report\\n            \\n\\n                AI in Action 2024\\n            \\nRead about 2,000 organizations we surveyed about their AI initiatives to discover what's working, what's not and how you can get ahead.\\n\\nRead the report\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n        \\n\\n     \\n    Related solutions\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Foundation models\"),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='Related solutions\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Foundation models\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nExplore Granite 3.2 and the IBM library of foundation models in the watsonx portfolio to scale generative AI for your business with confidence.\\n\\n\\n\\n\\nExplore watsonx.ai\\n                \\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Artificial intelligence solutions\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nPut AI to work in your business with IBM’s industry-leading AI expertise and portfolio of solutions at your side.\\n\\n\\n\\n\\nExplore AI solutions\\n                \\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    AI consulting and services\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nReinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value.'),\n",
       " Document(metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='AI consulting and services\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nReinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value.\\n\\n\\n\\n\\nExplore AI services\\n                \\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTake the next step\\n\\n\\n\\n\\nExplore the IBM library of foundation models in the IBM watsonx portfolio to scale generative AI for your business with confidence.\\n\\n\\n\\n\\n\\n\\nExplore watsonx.ai\\n\\n\\n\\n\\n\\nExplore AI solutions')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d50f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70d55dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eadad050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x17eb03500>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "915562aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows. LangChain’s module-based approach allows developers and data scientists to dynamically compare different prompts and even different foundation models with minimal need to rewrite code. This modular environment also allows for programs that use multiple LLMs: for example, an application that uses one LLM to interpret user queries and another LLM to author a response.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query From a vector db\n",
    "query=\"LangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows.\"\n",
    "result=vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ad850e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e634804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x3118605c0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x311861d60>, root_client=<openai.OpenAI object at 0x17ca99880>, root_async_client=<openai.AsyncOpenAI object at 0x17eb02450>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval Chain, Document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9228d5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How does LangChain benefit developers and data scientists when working with LLM applications?\\n\\nLangChain benefits developers and data scientists by offering a centralized development environment that simplifies the process of building LLM applications. Its module-based approach allows users to dynamically compare different prompts and foundation models, minimizing the need to rewrite code. This flexibility and efficiency make it easier to integrate LLMs with external data sources and software workflows.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"\",\n",
    "    \"context\":[Document(page_content=\"LangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows. LangChain’s module-based approach allows developers and data scientists to dynamically compare different prompts and even different foundation models with minimal need to rewrite code.\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677da3b3",
   "metadata": {},
   "source": [
    "However, we want the documents to first come from the retriever we just set up. That way, we can use the retriever to dynamically select the most relevant documents and pass those in for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "06dfea35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x17eb03500>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Input--->Retriever--->vectorstoredb\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c45259dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37eb865b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x17eb03500>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x3118605c0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x311861d60>, root_client=<openai.OpenAI object at 0x17ca99880>, root_async_client=<openai.AsyncOpenAI object at 0x17eb02450>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9e721545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LangChain works by providing a development environment that streamlines the programming of LLM applications through the use of abstraction. This involves representing complex processes as named components that encapsulate all their constituent steps. LangChain acts as a library of abstractions in Python and JavaScript, offering modular components like functions and object classes, which can be chained together to create generative AI applications. This modular, abstracted approach allows developers to dynamically compare different prompts and foundation models without rewriting much code. LangChain supports custom LLM wrappers and integrates with external models, such as those from IBM's watsonx, to build applications tailored to specific needs. While this abstracted approach might limit detailed customization by expert programmers, it simplifies the process for both specialists and newcomers, enabling quick experimentation and prototyping of LLM-based applications.\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the response form the LLM\n",
    "response=retrieval_chain.invoke({\"input\":\"LangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows. \"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "440146fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'LangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows. ',\n",
       " 'context': [Document(id='cbe9cbc9-3cc2-4dc3-9735-408fe66e87ba', metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='LangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows. LangChain’s module-based approach allows developers and data scientists to dynamically compare different prompts and even different foundation models with minimal need to rewrite code. This modular environment also allows for programs that use multiple LLMs: for example, an application that uses one LLM to interpret user queries and another LLM to author a response.'),\n",
       "  Document(id='a81fe579-d493-4e90-8e9d-8c40c18709f0', metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='How does LangChain work?\\r\\n    \\n\\n\\n\\nAt LangChain’s core is a development environment that streamlines the programming of LLM applications through the use of\\xa0abstraction: the simplification of code by representing one or more complex processes as a named component that encapsulates all of its constituent steps.'),\n",
       "  Document(id='4870ee82-743f-4785-b68c-03370cde26a5', metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='LangChain is not limited to out-of-the-box foundation models: the CustomLLM class\\xa0allows for custom LLM wrappers. Likewise, you can use the IBM watsonx APIs and Python SDK, which includes a LangChain integration, to build applications in LangChain with models that you’ve already trained or fine-tuned for your specific needs using the WatsonxLLM class (and that model’s specific project ID).'),\n",
       "  Document(id='60754f6a-bc09-4d17-b561-036755ca16aa', metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='LangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models. These modular components—like functions and object classes—serve as the building blocks of generative AI programs. They can be “chained” together to create applications, minimizing the amount of code and fine understanding required to execute complex NLP tasks. Though LangChain’s abstracted approach may limit the extent to which an expert programmer can finely customize an application, it empowers specialists and newcomers alike to quickly experiment and prototype.')],\n",
       " 'answer': \"LangChain works by providing a development environment that streamlines the programming of LLM applications through the use of abstraction. This involves representing complex processes as named components that encapsulate all their constituent steps. LangChain acts as a library of abstractions in Python and JavaScript, offering modular components like functions and object classes, which can be chained together to create generative AI applications. This modular, abstracted approach allows developers to dynamically compare different prompts and foundation models without rewriting much code. LangChain supports custom LLM wrappers and integrates with external models, such as those from IBM's watsonx, to build applications tailored to specific needs. While this abstracted approach might limit detailed customization by expert programmers, it simplifies the process for both specialists and newcomers, enabling quick experimentation and prototyping of LLM-based applications.\"}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef382513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='cbe9cbc9-3cc2-4dc3-9735-408fe66e87ba', metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='LangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows. LangChain’s module-based approach allows developers and data scientists to dynamically compare different prompts and even different foundation models with minimal need to rewrite code. This modular environment also allows for programs that use multiple LLMs: for example, an application that uses one LLM to interpret user queries and another LLM to author a response.'),\n",
       " Document(id='a81fe579-d493-4e90-8e9d-8c40c18709f0', metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='How does LangChain work?\\r\\n    \\n\\n\\n\\nAt LangChain’s core is a development environment that streamlines the programming of LLM applications through the use of\\xa0abstraction: the simplification of code by representing one or more complex processes as a named component that encapsulates all of its constituent steps.'),\n",
       " Document(id='4870ee82-743f-4785-b68c-03370cde26a5', metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='LangChain is not limited to out-of-the-box foundation models: the CustomLLM class\\xa0allows for custom LLM wrappers. Likewise, you can use the IBM watsonx APIs and Python SDK, which includes a LangChain integration, to build applications in LangChain with models that you’ve already trained or fine-tuned for your specific needs using the WatsonxLLM class (and that model’s specific project ID).'),\n",
       " Document(id='60754f6a-bc09-4d17-b561-036755ca16aa', metadata={'source': 'https://www.ibm.com/think/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content='LangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models. These modular components—like functions and object classes—serve as the building blocks of generative AI programs. They can be “chained” together to create applications, minimizing the amount of code and fine understanding required to execute complex NLP tasks. Though LangChain’s abstracted approach may limit the extent to which an expert programmer can finely customize an application, it empowers specialists and newcomers alike to quickly experiment and prototype.')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25290004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7a753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a412c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a2c2db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
